<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AI Particle Agent Canvas</title>
    <style>
        body { margin: 0; overflow: hidden; background: radial-gradient(circle at center, #0a0a0a 0%, #000 100%); color: white; font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif; }
        
        #ui-container {
            position: absolute;
            bottom: 30px;
            left: 50%;
            transform: translateX(-50%);
            width: 95%;
            max-width: 600px;
            display: flex;
            flex-direction: column;
            align-items: center;
            gap: 15px;
            z-index: 10;
        }

        .input-group {
            display: flex;
            width: 100%;
            background: rgba(0, 255, 255, 0.05);
            border: 1px solid rgba(0, 255, 255, 0.2);
            border-radius: 30px;
            padding: 5px 10px;
            backdrop-filter: blur(15px);
            box-shadow: 0 0 20px rgba(0,0,0,0.5);
        }

        input {
            flex: 1;
            background: none;
            border: none;
            color: #0ff;
            padding: 12px 15px;
            outline: none;
            font-size: 16px;
        }

        .icon-btn {
            background: rgba(0, 255, 255, 0.1);
            border: 1px solid rgba(0, 255, 255, 0.2);
            color: #0ff;
            width: 45px;
            height: 45px;
            border-radius: 50%;
            cursor: pointer;
            display: flex;
            justify-content: center;
            align-items: center;
            transition: 0.3s;
            margin-left: 5px;
        }

        .icon-btn:hover { background: rgba(0, 255, 255, 0.3); }
        .icon-btn.active { background: #f00; color: #fff; border-color: #f00; box-shadow: 0 0 15px #f00; }

        .status-tag {
            background: rgba(0, 255, 255, 0.05);
            border: 1px solid rgba(0, 255, 255, 0.2);
            padding: 5px 18px;
            border-radius: 20px;
            font-size: 11px;
            color: #0ff;
            text-transform: uppercase;
            letter-spacing: 2px;
        }

        #ai-response {
            font-size: 14px;
            color: #00d2d2;
            margin-bottom: 8px;
            text-align: center;
            max-height: 120px;
            overflow-y: auto;
            text-shadow: 0 0 5px rgba(0, 255, 255, 0.3);
            width: 100%;
        }

        #ui-layer { position: absolute; top: 25px; left: 25px; pointer-events: none; }
        h1 { margin: 0; font-weight: 200; font-size: 1rem; letter-spacing: 6px; color: #0ff; text-shadow: 0 0 10px rgba(0,255,255,0.5); }

        .loading-screen {
            position: fixed;
            top: 0; left: 0; width: 100%; height: 100%;
            background: black;
            display: flex;
            justify-content: center;
            align-items: center;
            z-index: 100;
            transition: opacity 1.2s;
        }
    </style>
</head>
<body>

    <div id="loading-screen" class="loading-screen">
        <div class="status-tag">SYNCHRONIZING NEURAL BRAIN...</div>
    </div>

    <div id="ui-layer">
        <h1>NEURAL_CONVERSATION_v2.0</h1>
    </div>

    <div id="ui-container">
        <div id="ai-response">Greetings. I am your neural agent. Ready to synchronize.</div>
        <div class="status-tag" id="status-text">SYSTEM READY</div>
        <div class="input-group">
            <button id="mic-btn" class="icon-btn" title="Voice Input">ðŸŽ¤</button>
            <input type="text" id="user-input" placeholder="Type or use mic to chat...">
            <button id="send-btn" class="icon-btn" style="border-radius: 20px; width: auto; padding: 0 20px;">Send</button>
        </div>
    </div>

    <script src="https://cdnjs.cloudflare.com/ajax/libs/three.js/r128/three.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/three@0.128.0/examples/js/loaders/GLTFLoader.js"></script>

    <script>
        let scene, camera, renderer, particles, originalPositions;
        let analyser, dataArray, audioCtx;
        let isSpeaking = false;
        const apiKey = ""; // Gemini API Key can be placed here
        
        const loader = new THREE.GLTFLoader();
        const modelURL = 'Meshy_AI_A_high_detailed_symm_0115153513_texture.glb';

        function init() {
            scene = new THREE.Scene();
            camera = new THREE.PerspectiveCamera(65, window.innerWidth / window.innerHeight, 0.1, 1000);
            camera.position.z = 4.5;

            renderer = new THREE.WebGLRenderer({ antialias: true, alpha: true });
            renderer.setSize(window.innerWidth, window.innerHeight);
            renderer.setPixelRatio(Math.min(window.devicePixelRatio, 2));
            document.body.appendChild(renderer.domElement);

            const ambientLight = new THREE.AmbientLight(0xffffff, 0.4);
            scene.add(ambientLight);
            
            const blueLight = new THREE.PointLight(0x00ffff, 1, 100);
            blueLight.position.set(10, 10, 10);
            scene.add(blueLight);

            loader.load(modelURL, 
                (gltf) => {
                    setupParticles(gltf.scene, true);
                    hideLoading();
                },
                undefined,
                (err) => {
                    setupParticles(new THREE.IcosahedronGeometry(1.8, 60), false);
                    hideLoading();
                }
            );

            animate();
        }

        function setupParticles(object, isScene) {
            let geometry;
            if(isScene) {
                object.traverse(child => { if(child.isMesh) { geometry = child.geometry; child.visible = false; } });
            } else { geometry = object; }
            if(!geometry) return;

            geometry.center();
            geometry.scale(1.8, 1.8, 1.8);
            originalPositions = geometry.attributes.position.array.slice();
            
            const pGeo = new THREE.BufferGeometry();
            pGeo.setAttribute('position', new THREE.BufferAttribute(new Float32Array(originalPositions), 3));
            
            const pMat = new THREE.PointsMaterial({
                color: 0x00ffff,
                size: 0.007, 
                transparent: true,
                opacity: 0.9,
                blending: THREE.AdditiveBlending,
                sizeAttenuation: true
            });

            particles = new THREE.Points(pGeo, pMat);
            scene.add(particles);
        }

        function hideLoading() {
            document.getElementById('loading-screen').style.opacity = '0';
            setTimeout(() => { document.getElementById('loading-screen').style.display = 'none'; }, 1200);
        }

        function animate() {
            requestAnimationFrame(animate);
            const time = performance.now() * 0.001;

            if (particles) {
                const posAttr = particles.geometry.attributes.position;
                const posArray = posAttr.array;
                
                let audioIntensity = 0;
                if(analyser && isSpeaking) {
                    analyser.getByteFrequencyData(dataArray);
                    audioIntensity = (dataArray.reduce((a, b) => a + b, 0) / dataArray.length) / 40; 
                }

                for (let i = 0; i < posArray.length; i += 3) {
                    const ox = originalPositions[i];
                    const oy = originalPositions[i+1];
                    const oz = originalPositions[i+2];

                    const noise = Math.sin(time * 1.5 + oy * 3) * (0.005 + audioIntensity * 0.6);
                    posArray[i] = ox + noise;
                    posArray[i+1] = oy + Math.cos(time * 1.2 + ox * 3) * noise;
                    posArray[i+2] = oz + Math.sin(time * 2.5 + ox * 2) * (noise * 2);
                }
                posAttr.needsUpdate = true;
                particles.rotation.y = Math.sin(time * 0.1) * 0.15;
            }
            renderer.render(scene, camera);
        }

        // --- AI & Voice Logic ---
        const sendBtn = document.getElementById('send-btn');
        const micBtn = document.getElementById('mic-btn');
        const userInput = document.getElementById('user-input');
        const statusText = document.getElementById('status-text');
        const aiResponseDiv = document.getElementById('ai-response');

        // Text to Speech Setup with Visualizer
        async function speak(text) {
            if (!audioCtx) {
                audioCtx = new (window.AudioContext || window.webkitAudioContext)();
                analyser = audioCtx.createAnalyser();
                analyser.fftSize = 256;
                dataArray = new Uint8Array(analyser.frequencyBinCount);
            }

            const utterance = new SpeechSynthesisUtterance(text);
            utterance.onstart = () => { isSpeaking = true; statusText.innerText = "TRANSMITTING..."; };
            utterance.onend = () => { isSpeaking = false; statusText.innerText = "SYSTEM READY"; };
            window.speechSynthesis.speak(utterance);
        }

        // Voice Recognition (Speech to Text)
        const Recognition = window.SpeechRecognition || window.webkitSpeechRecognition;
        if (Recognition) {
            const recognition = new Recognition();
            recognition.continuous = false;
            recognition.lang = 'en-US';

            micBtn.onclick = () => {
                recognition.start();
                micBtn.classList.add('active');
                statusText.innerText = "LISTENING...";
            };

            recognition.onresult = (event) => {
                const transcript = event.results[0][0].transcript;
                userInput.value = transcript;
                micBtn.classList.remove('active');
                handleAI();
            };

            recognition.onerror = () => {
                micBtn.classList.remove('active');
                statusText.innerText = "MIC ERROR";
            };
        }

        async function handleAI() {
            const prompt = userInput.value.trim();
            if(!prompt) return;

            statusText.innerText = "THINKING...";
            userInput.value = "";
            aiResponseDiv.innerText = "...";

            try {
                // Gemini API Call
                const response = await fetch(`https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key=${apiKey}`, {
                    method: 'POST',
                    headers: { 'Content-Type': 'application/json' },
                    body: JSON.stringify({
                        contents: [{ parts: [{ text: `You are a neural AI agent living inside a 3D mesh. Keep your answers concise, cool, and a bit futuristic. User says: ${prompt}` }] }]
                    })
                });

                const data = await response.json();
                let aiText = data.candidates?.[0]?.content?.parts?.[0]?.text || "Neural connection timed out.";
                
                // fallback if API key is missing
                if(data.error) aiText = "Neural link offline. Please configure API key for full brain access.";

                aiResponseDiv.innerText = aiText;
                speak(aiText);
            } catch (err) {
                aiResponseDiv.innerText = "Communication failure. Retrying...";
                statusText.innerText = "ERROR";
            }
        }

        sendBtn.addEventListener('click', handleAI);
        userInput.addEventListener('keypress', (e) => { if(e.key === 'Enter') handleAI(); });

        window.addEventListener('resize', () => {
            camera.aspect = window.innerWidth / window.innerHeight;
            camera.updateProjectionMatrix();
            renderer.setSize(window.innerWidth, window.innerHeight);
        });

        init();
    </script>
</body>
</html>
