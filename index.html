<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Nudara Core - Precision Lip Sync</title>
    <style>
        body { 
            margin: 0; 
            overflow: hidden; 
            background: #000; 
            color: white; 
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif; 
        }
        
        /* UI Elements Styling */
        #ui-container {
            position: absolute;
            bottom: 40px;
            left: 50%;
            transform: translateX(-50%);
            width: 100%;
            max-width: 500px;
            display: flex;
            flex-direction: column;
            align-items: center;
            z-index: 10;
        }

        #start-overlay {
            position: fixed;
            top: 0; 
            left: 0; 
            width: 100%; 
            height: 100%;
            background: #000;
            display: flex;
            flex-direction: column;
            justify-content: center;
            align-items: center;
            z-index: 200;
        }

        .start-btn {
            background: transparent;
            color: #0ff;
            border: 2px solid #0ff;
            padding: 15px 40px;
            font-size: 1.2rem;
            border-radius: 5px;
            cursor: pointer;
            text-transform: uppercase;
            letter-spacing: 5px;
            box-shadow: 0 0 15px rgba(0, 255, 255, 0.3);
            transition: 0.3s;
        }

        .start-btn:hover { 
            background: #0ff; 
            color: #000; 
            box-shadow: 0 0 40px #0ff; 
        }

        #ai-response {
            font-size: 18px;
            color: #0ff;
            margin-bottom: 20px;
            text-align: center;
            min-height: 60px;
            width: 85%;
            text-shadow: 0 0 10px rgba(0, 255, 255, 0.8);
            line-height: 1.4;
        }

        .mic-trigger {
            width: 65px;
            height: 65px;
            border: 1px solid #333;
            border-radius: 50%;
            display: flex;
            justify-content: center;
            align-items: center;
            cursor: pointer;
            transition: 0.3s;
            font-size: 26px;
            background: rgba(0,0,0,0.5);
        }

        .mic-trigger.active {
            border-color: #f00;
            box-shadow: 0 0 25px #f00;
            color: #f00;
        }
    </style>
</head>
<body>

    <!-- Initial Loading Screen -->
    <div id="start-overlay">
        <button class="start-btn" onclick="activateSystem()">Initialize Nudara</button>
        <div style="margin-top:20px; font-size:11px; color:#0ff; opacity:0.6; letter-spacing: 2px;">
            NEURAL SYNC INTERFACE v2.5
        </div>
    </div>

    <!-- Main Interaction UI -->
    <div id="ui-container">
        <div id="ai-response">SYSTEM READY</div>
        <div class="mic-trigger" id="mic-btn">ðŸŽ¤</div>
    </div>

    <script src="https://cdnjs.cloudflare.com/ajax/libs/three.js/r128/three.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/three@0.128.0/examples/js/loaders/GLTFLoader.js"></script>

    <script>
        // Global variables for the 3D Engine
        let scene, camera, renderer, particles, originalPositions;
        let audioCtx, analyser, dataArray;
        let isSpeaking = false;
        let isListening = false;
        let mouthWeights = []; 
        let animationId;

        const apiKey = ""; 
        const modelURL = 'Meshy_AI_A_high_detailed_symm_0115153513_texture.glb';

        // Initialize the 3D Scene
        function init() {
            scene = new THREE.Scene();
            camera = new THREE.PerspectiveCamera(50, window.innerWidth / window.innerHeight, 0.1, 1000);
            camera.position.z = 5;

            renderer = new THREE.WebGLRenderer({ antialias: true, alpha: true });
            renderer.setSize(window.innerWidth, window.innerHeight);
            renderer.setPixelRatio(window.devicePixelRatio);
            document.body.appendChild(renderer.domElement);

            // Lighting setup for depth
            const ambientLight = new THREE.AmbientLight(0xffffff, 0.6);
            scene.add(ambientLight);

            const pointLight = new THREE.PointLight(0x00ffff, 1);
            pointLight.position.set(5, 5, 5);
            scene.add(pointLight);

            // Load the 3D Model and convert to Particles
            const loader = new THREE.GLTFLoader();
            loader.load(modelURL, (gltf) => {
                let mesh;
                gltf.scene.traverse(child => { 
                    if(child.isMesh) mesh = child; 
                });
                if(mesh) createParticleFace(mesh.geometry);
            }, undefined, () => {
                // Fallback geometry if model fails
                console.warn("Using fallback geometry.");
                createParticleFace(new THREE.IcosahedronGeometry(2, 64));
            });

            animate();
        }

        // Particle Generation & Lip-Weight Mapping
        function createParticleFace(geometry) {
            geometry.center();
            geometry.scale(2.2, 2.2, 2.2);
            
            const positions = geometry.attributes.position.array;
            originalPositions = new Float32Array(positions);
            
            const pGeo = new THREE.BufferGeometry();
            pGeo.setAttribute('position', new THREE.BufferAttribute(new Float32Array(positions), 3));
            
            mouthWeights = []; 
            
            // Map each particle's sensitivity to sound
            for (let i = 0; i < positions.length; i += 3) {
                const x = positions[i];
                const y = positions[i+1];
                const z = positions[i+2];
                
                let weight = 0;
                
                // Detection box for the mouth area
                const inMouthHeight = (y < 0.1 && y > -0.9);
                const inMouthWidth = (Math.abs(x) < 0.45);
                
                if (inMouthHeight && inMouthWidth) {
                    // Calculate influence based on proximity to lip center
                    const verticalDist = Math.abs(y + 0.35) / 0.6;
                    const horizontalDist = Math.abs(x) / 0.45;
                    
                    weight = Math.pow(1 - verticalDist, 2.5) * (1 - horizontalDist);
                }
                
                mouthWeights.push(Math.max(0, weight));
            }

            const pMat = new THREE.PointsMaterial({
                color: 0x00ffff,
                size: 0.018,
                transparent: true,
                opacity: 0.9,
                blending: THREE.AdditiveBlending,
                sizeAttenuation: true
            });

            particles = new THREE.Points(pGeo, pMat);
            scene.add(particles);
        }

        // Main Animation Loop
        function animate() {
            animationId = requestAnimationFrame(animate);
            const time = performance.now() * 0.001;

            if (particles) {
                updateParticlePositions(time);
                particles.rotation.y = Math.sin(time * 0.15) * 0.06;
            }
            
            renderer.render(scene, camera);
        }

        // Updates each particle based on voice and weight
        function updateParticlePositions(time) {
            const posAttr = particles.geometry.attributes.position;
            const posArray = posAttr.array;
            
            let volume = 0;
            if (isSpeaking && dataArray) {
                let sum = 0;
                for(let i = 0; i < 15; i++) sum += dataArray[i]; 
                volume = (sum / 15 / 255) * 2.0; // Audio Boost
            }

            for (let i = 0; i < posArray.length; i += 3) {
                const idx = i / 3;
                const weight = mouthWeights[idx];
                
                const ox = originalPositions[i];
                const oy = originalPositions[i+1];
                const oz = originalPositions[i+2];

                if (weight > 0) {
                    // Lip-Sync Movement (Isolated to Mouth)
                    const movementAmt = volume * weight * 0.75;
                    posArray[i+1] = oy - movementAmt;
                    posArray[i+2] = oz + (movementAmt * 0.35);
                    
                    // Speech Jitter
                    if (volume > 0.1) {
                        posArray[i] = ox + (Math.sin(time * 55 + i) * 0.005 * weight);
                    }
                } else {
                    // Idle breathing/drift for the rest of the face
                    posArray[i+1] = oy + Math.sin(time * 1.0 + ox) * 0.003;
                }
            }
            posAttr.needsUpdate = true;
        }

        // System Activation & Audio Setup
        async function activateSystem() {
            document.getElementById('start-overlay').style.display = 'none';
            
            if (!audioCtx) {
                audioCtx = new (window.AudioContext || window.webkitAudioContext)();
                analyser = audioCtx.createAnalyser();
                analyser.fftSize = 256;
                dataArray = new Uint8Array(analyser.frequencyBinCount);
            }
            
            if (audioCtx.state === 'suspended') {
                await audioCtx.resume();
            }

            speak("Calibration complete. Neural articulation centers are now isolated.");
            initSpeechRecognition();
        }

        // Speech Synthesis Logic
        function speak(text) {
            window.speechSynthesis.cancel();
            const utterance = new SpeechSynthesisUtterance(text);
            
            utterance.onstart = () => { 
                isSpeaking = true; 
                simulateMouthData();
            };
            
            utterance.onend = () => { 
                isSpeaking = false; 
            };
            
            window.speechSynthesis.speak(utterance);
        }

        // Simulate frequency data to drive lip movement
        function simulateMouthData() {
            if (!isSpeaking) return;
            
            const baseFreq = 0.4 + Math.sin(Date.now() * 0.02) * 0.4;
            const noise = Math.random() * 0.3;
            const currentVol = Math.min(1.0, baseFreq + noise);
            
            if (dataArray) {
                for(let i=0; i<32; i++) {
                    dataArray[i] = currentVol * 255;
                }
            }
            
            // Sync with animation frame
            setTimeout(() => {
                requestAnimationFrame(simulateMouthData);
            }, 30);
        }

        // AI Logic & Mic Input
        function initSpeechRecognition() {
            const Recognition = window.SpeechRecognition || window.webkitSpeechRecognition;
            if (!Recognition) return;
            
            const rec = new Recognition();
            rec.lang = 'en-US';
            
            document.getElementById('mic-btn').onclick = () => {
                isListening = !isListening;
                document.getElementById('mic-btn').classList.toggle('active');
                if (isListening) rec.start(); else rec.stop();
            };

            rec.onresult = (e) => {
                const text = e.results[0][0].transcript;
                processRequest(text);
            };
            
            rec.onend = () => { 
                if (isListening) rec.start(); 
            };
        }

        async function processRequest(text) {
            document.getElementById('ai-response').innerText = "...";
            
            const response = await fetch(`https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-preview-09-2025:generateContent?key=${apiKey}`, {
                method: 'POST',
                headers: { 'Content-Type': 'application/json' },
                body: JSON.stringify({
                    contents: [{ parts: [{ text: text }] }],
                    systemInstruction: { parts: [{ text: "Respond in 1 short sentence." }] }
                })
            });
            
            const data = await response.json();
            const aiText = data.candidates[0].content.parts[0].text;
            
            document.getElementById('ai-response').innerText = aiText;
            speak(aiText);
        }

        window.addEventListener('resize', () => {
            camera.aspect = window.innerWidth / window.innerHeight;
            camera.updateProjectionMatrix();
            renderer.setSize(window.innerWidth, window.innerHeight);
        });

        // Bootstrap the app
        init();
    </script>
</body>
</html>
