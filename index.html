<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Nudara Core - Precision Lip Sync</title>
    <style>
        body { margin: 0; overflow: hidden; background: #000; color: white; font-family: 'Courier New', Courier, monospace; }
        
        #ui-container {
            position: absolute;
            bottom: 40px;
            left: 50%;
            transform: translateX(-50%);
            width: 100%;
            max-width: 500px;
            display: flex;
            flex-direction: column;
            align-items: center;
            z-index: 10;
        }

        #start-overlay {
            position: fixed;
            top: 0; left: 0; width: 100%; height: 100%;
            background: #000;
            display: flex;
            flex-direction: column;
            justify-content: center;
            align-items: center;
            z-index: 200;
        }

        .start-btn {
            background: transparent;
            color: #0ff;
            border: 2px solid #0ff;
            padding: 15px 40px;
            font-size: 1.2rem;
            border-radius: 5px;
            cursor: pointer;
            text-transform: uppercase;
            letter-spacing: 5px;
            box-shadow: 0 0 15px rgba(0, 255, 255, 0.3);
            transition: 0.3s;
        }

        .start-btn:hover { background: #0ff; color: #000; box-shadow: 0 0 40px #0ff; }

        .status-tag {
            font-size: 10px;
            color: #0ff;
            margin-top: 15px;
            letter-spacing: 2px;
            opacity: 0.7;
        }

        #ai-response {
            font-size: 18px;
            color: #0ff;
            margin-bottom: 20px;
            text-align: center;
            min-height: 50px;
            width: 80%;
            text-shadow: 0 0 10px rgba(0, 255, 255, 0.8);
        }

        #status-display {
            font-size: 12px;
            color: #555;
            margin-bottom: 10px;
            text-transform: uppercase;
        }

        .mic-trigger {
            width: 60px;
            height: 60px;
            border: 1px solid #333;
            border-radius: 50%;
            display: flex;
            justify-content: center;
            align-items: center;
            cursor: pointer;
            transition: 0.3s;
        }

        .mic-trigger.active {
            border-color: #f00;
            box-shadow: 0 0 20px #f00;
            color: #f00;
        }
    </style>
</head>
<body>

    <div id="start-overlay">
        <button class="start-btn" onclick="activateSystem()">Initialize Nudara</button>
        <div class="status-tag">NEURAL LINK PENDING...</div>
    </div>

    <div id="ui-container">
        <div id="ai-response">READY</div>
        <div id="status-display">IDLE</div>
        <div class="mic-trigger" id="mic-btn">ðŸŽ¤</div>
    </div>

    <script src="https://cdnjs.cloudflare.com/ajax/libs/three.js/r128/three.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/three@0.128.0/examples/js/loaders/GLTFLoader.js"></script>

    <script>
        let scene, camera, renderer, particles, originalPositions;
        let audioCtx, analyser, dataArray;
        let isSpeaking = false;
        let isListening = false;
        let mouthWeights = []; // Stores how much each particle should move

        const apiKey = ""; // API Key is handled by environment
        const modelURL = 'Meshy_AI_A_high_detailed_symm_0115153513_texture.glb';

        function init() {
            scene = new THREE.Scene();
            camera = new THREE.PerspectiveCamera(50, window.innerWidth / window.innerHeight, 0.1, 1000);
            camera.position.z = 5;

            renderer = new THREE.WebGLRenderer({ antialias: true, alpha: true });
            renderer.setSize(window.innerWidth, window.innerHeight);
            document.body.appendChild(renderer.domElement);

            const light = new THREE.DirectionalLight(0xffffff, 1);
            light.position.set(0, 0, 10);
            scene.add(light);

            const loader = new THREE.GLTFLoader();
            loader.load(modelURL, (gltf) => {
                let mesh;
                gltf.scene.traverse(child => { if(child.isMesh) mesh = child; });
                createParticleFace(mesh.geometry);
                document.getElementById('start-overlay').querySelector('.status-tag').innerText = "SYSTEM LOADED";
            }, undefined, () => {
                createParticleFace(new THREE.IcosahedronGeometry(2, 60));
            });

            animate();
        }

        function createParticleFace(geometry) {
            geometry.center();
            geometry.scale(2.2, 2.2, 2.2);
            
            const positions = geometry.attributes.position.array;
            originalPositions = new Float32Array(positions);
            
            const pGeo = new THREE.BufferGeometry();
            pGeo.setAttribute('position', new THREE.BufferAttribute(new Float32Array(positions), 3));
            
            // Calculate weights for each particle based on its position
            // This ensures only the mouth/jaw area moves significantly
            for (let i = 0; i < positions.length; i += 3) {
                const y = positions[i+1];
                const x = positions[i];
                const z = positions[i+2];

                // Influence Map: High value for mouth/chin area, low for eyes/forehead
                // We focus on Y between -0.8 and 0.1, and X within center
                let weight = 0;
                if (y < 0.2 && y > -1.0 && Math.abs(x) < 0.5) {
                    // Normalize weight: 1.0 at the center of the lips, tapering off
                    weight = Math.pow(1 - (Math.abs(y + 0.3) / 0.7), 2) * (1 - Math.abs(x) / 0.5);
                }
                mouthWeights.push(Math.max(0, weight));
            }

            const pMat = new THREE.PointsMaterial({
                color: 0x00ffff,
                size: 0.012,
                transparent: true,
                opacity: 0.9,
                blending: THREE.AdditiveBlending
            });

            particles = new THREE.Points(pGeo, pMat);
            scene.add(particles);
        }

        function animate() {
            requestAnimationFrame(animate);
            const time = performance.now() * 0.001;

            if (particles) {
                const posAttr = particles.geometry.attributes.position;
                const posArray = posAttr.array;
                
                let volume = 0;
                if (isSpeaking && analyser) {
                    analyser.getByteFrequencyData(dataArray);
                    let sum = 0;
                    for(let i = 0; i < 32; i++) sum += dataArray[i]; // Focus on lower frequencies (vowels)
                    volume = sum / 32 / 255;
                }

                for (let i = 0; i < posArray.length; i += 3) {
                    const idx = i / 3;
                    const weight = mouthWeights[idx];
                    
                    const ox = originalPositions[i];
                    const oy = originalPositions[i+1];
                    const oz = originalPositions[i+2];

                    if (weight > 0) {
                        // Move mouth particles DOWN based on volume and weight
                        const offset = volume * weight * 0.6; 
                        posArray[i+1] = oy - offset;
                        posArray[i+2] = oz + (offset * 0.3); // Slight forward movement for depth
                        
                        // Add jitter/vibration for realistic "talking" look
                        if (volume > 0.1) {
                            posArray[i] = ox + (Math.sin(time * 50 + i) * 0.005 * weight);
                        }
                    } else {
                        // Subtle idle movement for the rest of the face
                        posArray[i+1] = oy + Math.sin(time + ox) * 0.005;
                    }
                }
                posAttr.needsUpdate = true;
                particles.rotation.y = Math.sin(time * 0.2) * 0.05;
            }
            renderer.render(scene, camera);
        }

        function activateSystem() {
            document.getElementById('start-overlay').style.display = 'none';
            if (!audioCtx) {
                audioCtx = new (window.AudioContext || window.webkitAudioContext)();
                analyser = audioCtx.createAnalyser();
                analyser.fftSize = 256;
                dataArray = new Uint8Array(analyser.frequencyBinCount);
            }
            speak("Neural interface synchronized. Focusing all energy on precision speech articulation.");
            initSpeechRecognition();
        }

        function speak(text) {
            window.speechSynthesis.cancel();
            const utterance = new SpeechSynthesisUtterance(text);
            utterance.onstart = () => { 
                isSpeaking = true; 
                document.getElementById('status-display').innerText = "TRANSMITTING...";
                simulateMouthData();
            };
            utterance.onend = () => { 
                isSpeaking = false; 
                document.getElementById('status-display').innerText = isListening ? "LISTENING" : "IDLE";
            };
            window.speechSynthesis.speak(utterance);
        }

        // Simulates audio data since SS is hard to pipe
        function simulateMouthData() {
            if (!isSpeaking) return;
            // Generate rhythmic volume changes to mimic speech patterns
            const vol = 0.4 + Math.sin(Date.now() * 0.015) * 0.4;
            for(let i=0; i<32; i++) dataArray[i] = vol * 255;
            requestAnimationFrame(simulateMouthData);
        }

        function initSpeechRecognition() {
            const Recognition = window.SpeechRecognition || window.webkitSpeechRecognition;
            if (!Recognition) return;
            const rec = new Recognition();
            rec.lang = 'en-US';
            
            document.getElementById('mic-btn').onclick = () => {
                isListening = !isListening;
                document.getElementById('mic-btn').classList.toggle('active');
                if (isListening) rec.start(); else rec.stop();
            };

            rec.onresult = (e) => {
                const text = e.results[0][0].transcript;
                processRequest(text);
            };
            
            rec.onend = () => { if (isListening) rec.start(); };
        }

        async function processRequest(text) {
            document.getElementById('status-display').innerText = "THINKING";
            const response = await fetch(`https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-preview-09-2025:generateContent?key=${apiKey}`, {
                method: 'POST',
                headers: { 'Content-Type': 'application/json' },
                body: JSON.stringify({
                    contents: [{ parts: [{ text: text }] }],
                    systemInstruction: { parts: [{ text: "Short 1 sentence replies only." }] }
                })
            });
            const data = await response.json();
            const aiText = data.candidates[0].content.parts[0].text;
            document.getElementById('ai-response').innerText = aiText;
            speak(aiText);
        }

        window.addEventListener('resize', () => {
            camera.aspect = window.innerWidth / window.innerHeight;
            camera.updateProjectionMatrix();
            renderer.setSize(window.innerWidth, window.innerHeight);
        });

        init();
    </script>
</body>
</html>
